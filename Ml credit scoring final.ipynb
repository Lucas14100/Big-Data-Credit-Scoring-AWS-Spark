{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Classification du Risque de Faillite de Prêt\n## Home Credit Default Risk — Modélisation ML sur Apache Spark\n\n---\n\n**Contexte :** Dans ce notebook, nous entraînons 3 modèles de Machine Learning sur Spark pour prédire le risque de défaut de remboursement d'un prêt (variable cible : `TARGET`).\n\n**Données :** Fichier Parquet fusionné issu des Jobs Glue (application_train + bureau + bureau_balance + previous_application + installments_payments + POS_CASH_balance + credit_card_balance)\n\n**Modèles entraînés :**\n- Régression Logistique\n- Random Forest\n- Gradient Boosted Trees (GBT — équivalent XGBoost sur Spark)\n\n**Pipeline ML :** Imputation → Encodage → Assemblage → Standardisation → Modèle",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "---\n## Partie 1 — Initialisation et Chargement des Données",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### 1.1 — Initialisation de la session Spark",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import re\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\nspark = SparkSession.builder.appName('ML_Credit_Scoring').getOrCreate()\nprint('Session Spark démarrée !')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 1.2 — Chargement du fichier Parquet fusionné (output Job Glue)",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df = spark.read.parquet('s3://projet-big-data-credit-beloin-lucas/Sortie Job3/')\nprint(f'Dimensions : {df.count()} lignes x {len(df.columns)} colonnes')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "---\n## Partie 2 — Préparation et Nettoyage des Données",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### 2.1 — Nettoyage des noms de colonnes\n\nLes colonnes issues des Jobs Glue contiennent des caractères spéciaux (`#`, `()`, `.`) qui doivent être supprimés pour être compatibles avec Spark ML.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def clean_column_name(name):\n    clean = re.sub(r'[^a-zA-Z0-9]', '_', name)\n    clean = re.sub(r'_+', '_', clean).strip('_')\n    if not clean:\n        clean = 'col_unknown'\n    return clean\n\nseen = {}\nfor col_name in df.columns:\n    new_name = clean_column_name(col_name)\n    if new_name in seen:\n        seen[new_name] += 1\n        new_name = f\"{new_name}_{seen[new_name]}\"\n    else:\n        seen[new_name] = 0\n    if new_name != col_name:\n        df = df.withColumnRenamed(col_name, new_name)\n\nprint('Colonnes nettoyées !')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 2.2 — Cast des colonnes numériques\n\nLes fichiers CSV lus par Glue sont souvent importés en `string`. On convertit ici toutes les variables numériques en `double` pour que Spark ML puisse les traiter correctement.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "cols_to_cast = [\n    'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE',\n    'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION',\n    'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE',\n    'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS',\n    'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'HOUR_APPR_PROCESS_START',\n    'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n    'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY',\n    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n    'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG',\n    'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG',\n    'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG',\n    'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE',\n    'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE',\n    'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n    'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI',\n    'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI',\n    'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI',\n    'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI',\n    'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n    'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE',\n    'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n    'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',\n    'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13',\n    'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n    'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21',\n    'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n    'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'\n]\n\nfor c in cols_to_cast:\n    if c in df.columns:\n        df = df.withColumn(c, F.col(c).cast('double'))\n\ndf = df.withColumn('TARGET', F.col('TARGET').cast('int'))\nprint('Cast effectué !')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 2.3 — Identification des features et traitement des valeurs manquantes\n\n- **Variables numériques** : imputation par la médiane (via `Imputer`)\n- **Variables catégorielles** : remplacement des valeurs nulles par `'Unknown'`\n\n> **Note :** Les NULLs sont normaux sur ce dataset — ils proviennent des Left Joins (clients sans historique bureau, sans carte de crédit, etc.)",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "excluded_cols = ['TARGET', 'SK_ID_CURR', 'right_sk_id_curr']\n\nnumeric_features = [\n    f.name for f in df.schema.fields\n    if isinstance(f.dataType, (DoubleType, IntegerType, LongType))\n    and f.name not in excluded_cols\n]\n\ncategorical_features = [\n    f.name for f in df.schema.fields\n    if isinstance(f.dataType, StringType)\n    and f.name not in excluded_cols\n]\n\n# Remplacement des nulls dans les catégorielles\nfor c in categorical_features:\n    df = df.withColumn(c, F.when(\n        F.col(c).isNull() | (F.col(c) == ''), 'Unknown'\n    ).otherwise(F.col(c)))\n\nprint(f'Variables numériques    : {len(numeric_features)}')\nprint(f'Variables catégorielles : {len(categorical_features)}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 2.4 — Split Train / Test (80% / 20%)\n\nLe split est stratifié via `seed=42` pour garantir la reproductibilité. Le taux de défaut (~8%) doit être similaire dans les deux sous-ensembles.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\nprint(f'Train : {train_df.count()} lignes')\nprint(f'Test  : {test_df.count()} lignes')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "---\n## Partie 3 — Construction du Pipeline ML\n\nLe pipeline Spark ML enchaîne automatiquement toutes les étapes de transformation :\n\n1. **`Imputer`** — remplace les NULLs numériques par la médiane\n2. **`StringIndexer`** — convertit les catégories texte en indices numériques\n3. **`OneHotEncoder`** — transforme les indices en vecteurs binaires\n4. **`VectorAssembler`** — assemble toutes les features en un seul vecteur\n5. **`StandardScaler`** — normalise les features (utile pour la Régression Logistique)",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer\n\n# 1. Imputation des valeurs manquantes (médiane)\nimputer = Imputer(\n    inputCols=numeric_features,\n    outputCols=[c + '_imputed' for c in numeric_features],\n    strategy='median'\n)\n\n# 2. Encodage des variables catégorielles\nindexers = [\n    StringIndexer(inputCol=c, outputCol=c + '_idx', handleInvalid='keep')\n    for c in categorical_features\n]\n\nencoders = [\n    OneHotEncoder(inputCol=c + '_idx', outputCol=c + '_vec')\n    for c in categorical_features\n]\n\n# 3. Assemblage de toutes les features en un seul vecteur\nassembler_inputs = (\n    [c + '_imputed' for c in numeric_features] +\n    [c + '_vec' for c in categorical_features]\n)\n\nassembler = VectorAssembler(\n    inputCols=assembler_inputs,\n    outputCol='assembled_features',\n    handleInvalid='keep'\n)\n\n# 4. Standardisation (nécessaire pour la Régression Logistique)\nscaler = StandardScaler(\n    inputCol='assembled_features',\n    outputCol='features',\n    withStd=True,\n    withMean=False  # False car vecteurs sparse (One-Hot)\n)\n\nprint('Pipeline configuré !')\nprint(f'Total features en entrée : {len(assembler_inputs)}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "---\n## Partie 4 — Modèle 1 : Régression Logistique\n\nLa régression logistique est un modèle linéaire de référence pour la classification binaire. Elle est rapide à entraîner et facilement interprétable.\n\n**Hyperparamètres :**\n- `maxIter=10` : nombre d'itérations de la descente de gradient\n- `regParam=0.01` : régularisation L2 (Ridge) pour éviter le surapprentissage\n- `elasticNetParam=0.0` : 0 = Ridge pur, 1 = Lasso",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nlr = LogisticRegression(\n    featuresCol='features',\n    labelCol='TARGET',\n    maxIter=10,\n    regParam=0.01,\n    elasticNetParam=0.0\n)\n\npipeline_lr = Pipeline(\n    stages=[imputer] + indexers + encoders + [assembler, scaler, lr]\n)\n\nmodel_lr = pipeline_lr.fit(train_df)\nprint('Modèle entraîné !')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 4.1 — Évaluation de la Régression Logistique\n\n**Métriques utilisées :**\n- **AUC-ROC** : métrique principale — mesure la capacité du modèle à distinguer les défauts des non-défauts (1.0 = parfait, 0.5 = aléatoire)\n- **Accuracy** : trompeuse ici car le dataset est déséquilibré (8% de défauts)\n- **F1-Score** : compromis précision/rappel — plus fiable sur données déséquilibrées\n- **Précision** : parmi les défauts prédits, combien sont réels ?\n- **Rappel** : parmi les vrais défauts, combien sont détectés ?",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\npreds_lr = model_lr.transform(test_df)\n\nauc_eval  = BinaryClassificationEvaluator(labelCol='TARGET', metricName='areaUnderROC')\nacc_eval  = MulticlassClassificationEvaluator(labelCol='TARGET', predictionCol='prediction', metricName='accuracy')\nf1_eval   = MulticlassClassificationEvaluator(labelCol='TARGET', predictionCol='prediction', metricName='f1')\nprec_eval = MulticlassClassificationEvaluator(labelCol='TARGET', predictionCol='prediction', metricName='weightedPrecision')\nrec_eval  = MulticlassClassificationEvaluator(labelCol='TARGET', predictionCol='prediction', metricName='weightedRecall')\n\nauc_lr  = auc_eval.evaluate(preds_lr)\nacc_lr  = acc_eval.evaluate(preds_lr)\nf1_lr   = f1_eval.evaluate(preds_lr)\nprec_lr = prec_eval.evaluate(preds_lr)\nrec_lr  = rec_eval.evaluate(preds_lr)\n\nprint('=== Régression Logistique ===')\nprint(f'AUC-ROC   : {auc_lr:.4f}')\nprint(f'Accuracy  : {acc_lr:.4f}')\nprint(f'F1-Score  : {f1_lr:.4f}')\nprint(f'Précision : {prec_lr:.4f}')\nprint(f'Rappel    : {rec_lr:.4f}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "---\n## Partie 5 — Modèle 2 : Random Forest\n\nLe Random Forest est un ensemble de arbres de décision entraînés sur des sous-échantillons aléatoires. Il est robuste au surapprentissage et ne nécessite pas de standardisation.\n\n**Hyperparamètres :**\n- `numTrees=100` : nombre d'arbres dans la forêt\n- `maxDepth=10` : profondeur maximale de chaque arbre\n\n> **Note :** Le `StandardScaler` n'est pas utilisé pour le Random Forest — les arbres de décision sont insensibles à l'échelle des variables.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    featuresCol='assembled_features',\n    labelCol='TARGET',\n    numTrees=100,\n    maxDepth=10,\n    seed=42\n)\n\n# Pas besoin de scaler pour Random Forest\npipeline_rf = Pipeline(\n    stages=[imputer] + indexers + encoders + [assembler, rf]\n)\n\nmodel_rf = pipeline_rf.fit(train_df)\nprint('Modèle entraîné !')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 5.1 — Évaluation du Random Forest",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "preds_rf = model_rf.transform(test_df)\n\nauc_rf  = auc_eval.evaluate(preds_rf)\nacc_rf  = acc_eval.evaluate(preds_rf)\nf1_rf   = f1_eval.evaluate(preds_rf)\nprec_rf = prec_eval.evaluate(preds_rf)\nrec_rf  = rec_eval.evaluate(preds_rf)\n\nprint('=== Random Forest ===')\nprint(f'AUC-ROC   : {auc_rf:.4f}')\nprint(f'Accuracy  : {acc_rf:.4f}')\nprint(f'F1-Score  : {f1_rf:.4f}')\nprint(f'Précision : {prec_rf:.4f}')\nprint(f'Rappel    : {rec_rf:.4f}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "---\n## Partie 6 — Modèle 3 : Gradient Boosted Trees (GBT / XGBoost)\n\nLe GBT est un algorithme de boosting qui construit les arbres séquentiellement, chaque arbre corrigeant les erreurs du précédent. C'est l'équivalent Spark natif de XGBoost.\n\n**Hyperparamètres :**\n- `maxIter=100` : nombre d'arbres (itérations de boosting)\n- `maxDepth=5` : profondeur maximale — plus faible que RF pour éviter le surapprentissage\n\n> **Note :** Le GBT est généralement le plus performant des 3 sur des données tabulaires, mais aussi le plus lent à entraîner.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier(\n    featuresCol='assembled_features',\n    labelCol='TARGET',\n    maxIter=100,\n    maxDepth=5,\n    seed=42\n)\n\npipeline_gbt = Pipeline(\n    stages=[imputer] + indexers + encoders + [assembler, gbt]\n)\n\nmodel_gbt = pipeline_gbt.fit(train_df)\nprint('Modèle entraîné !')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 6.1 — Évaluation du GBT",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "preds_gbt = model_gbt.transform(test_df)\n\nauc_gbt  = auc_eval.evaluate(preds_gbt)\nacc_gbt  = acc_eval.evaluate(preds_gbt)\nf1_gbt   = f1_eval.evaluate(preds_gbt)\nprec_gbt = prec_eval.evaluate(preds_gbt)\nrec_gbt  = rec_eval.evaluate(preds_gbt)\n\nprint('=== GBT (XGBoost) ===')\nprint(f'AUC-ROC   : {auc_gbt:.4f}')\nprint(f'Accuracy  : {acc_gbt:.4f}')\nprint(f'F1-Score  : {f1_gbt:.4f}')\nprint(f'Précision : {prec_gbt:.4f}')\nprint(f'Rappel    : {rec_gbt:.4f}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "---\n## Partie 7 — Comparaison et Analyse des Résultats\n\n### 7.1 — Tableau comparatif des 3 modèles",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "print('=' * 65)\nprint(f'{\"Modèle\":<25} {\"AUC-ROC\":>8} {\"Accuracy\":>10} {\"F1\":>8} {\"Précision\":>10} {\"Rappel\":>8}')\nprint('=' * 65)\nprint(f'{\"Régression Logistique\":<25} {auc_lr:>8.4f} {acc_lr:>10.4f} {f1_lr:>8.4f} {prec_lr:>10.4f} {rec_lr:>8.4f}')\nprint(f'{\"Random Forest\":<25} {auc_rf:>8.4f} {acc_rf:>10.4f} {f1_rf:>8.4f} {prec_rf:>10.4f} {rec_rf:>8.4f}')\nprint(f'{\"GBT (XGBoost)\":<25} {auc_gbt:>8.4f} {acc_gbt:>10.4f} {f1_gbt:>8.4f} {prec_gbt:>10.4f} {rec_gbt:>8.4f}')\nprint('=' * 65)\nprint()\nprint('Meilleur modèle par AUC-ROC : GBT (0.7530)')\nprint('NOTE : AUC-ROC et F1 sont les métriques clés car TARGET est déséquilibré (8% de défauts)')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### 7.2 — Matrice de confusion du meilleur modèle (GBT)\n\nLa matrice de confusion permet de visualiser les types d'erreurs du modèle :\n- **TP (True Positive)** : défauts correctement détectés → crucial en finance\n- **TN (True Negative)** : non-défauts correctement rejetés\n- **FP (False Positive)** : non-défauts classés comme défauts → coût commercial\n- **FN (False Negative)** : défauts manqués → coût financier le plus grave",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "print('Matrice de confusion — GBT (meilleur modèle) :')\npreds_gbt.groupBy('TARGET', 'prediction').count().orderBy('TARGET', 'prediction').show()\n\ntp = preds_gbt.filter((F.col('TARGET') == 1) & (F.col('prediction') == 1)).count()\ntn = preds_gbt.filter((F.col('TARGET') == 0) & (F.col('prediction') == 0)).count()\nfp = preds_gbt.filter((F.col('TARGET') == 0) & (F.col('prediction') == 1)).count()\nfn = preds_gbt.filter((F.col('TARGET') == 1) & (F.col('prediction') == 0)).count()\n\nprint(f'TP (vrais positifs)  : {tp}  — défauts bien détectés')\nprint(f'TN (vrais négatifs)  : {tn}  — non-défauts bien détectés')\nprint(f'FP (faux positifs)   : {fp}  — non-défauts classés comme défauts')\nprint(f'FN (faux négatifs)   : {fn}  — défauts manqués')\nprint()\nprint(f'Sensibilité (Recall) : {tp/(tp+fn):.4f}')\nprint(f'Spécificité          : {tn/(tn+fp):.4f}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}